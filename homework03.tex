\documentclass[11pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}

\def\N{{\mathbb N}}
\def\NN{{\mathcal N}}
\def\R{{\mathbb R}}
\def\E{{\mathbb E}}
\def\tr{{\mathrm{trace}}}
\def\P{{\mathrm{Prob}}}
\def\sign{{\mathrm{sign}}}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0.25 in}
\setlength{\parskip}{0.1 in}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \setcounter{section}{0}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf A Mathematical Introduction to Data Science \hfill #4} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\it Instructor: #2\hfill #3} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}


\begin{document}

\lecture{Homework 3. Stein's Phenomenon}{Yuan Yao}{Due: Tuesday October 21, 2014}{October 14, 2014}

The problem below marked by $^*$ is optional with bonus credits. % For the experimental problem, include the source codes which are runnable under standard settings. 

\begin{enumerate}

\item {\em Maximum Likelihood Method}: consider $n$ random samples from a multivariate normal distribution, $X_i \in \R^p \sim \NN(\mu,\Sigma)$ with $i=1,\ldots,n$.  

\begin{enumerate}
\item Show the log-likelihood function
\[ l_n(\mu,\Sigma) = - \frac{n}{2} \tr(\Sigma^{-1} S_n ) - \frac{n}{2} \log \det(\Sigma) + C, \]
where $S_n = \frac{1}{n}\sum_{i=1}^n (X_i - \mu)(X_i - \mu)^T$, and some constant $C$ does not depend on $\mu$ and $\Sigma$; 
\item Show that $f(X) = \tr(AX^{-1})$ with $A, X\succeq 0$ has a first-order approximation,
\[ f(X+\Delta) \approx f(X) - \tr(X^{-1}A' X^{-1} \Delta ) \]
hence formally $d f(X) / d X = - X^{-1} A X^{-1}$ (note $(I+X)^{-1} \approx I - X$. A typo in previous version missed `-' sign here.);  
\item Show that $g(X) = \log\det(X)$ with $A, X\succeq 0$ has a first-order approximation,
\[ g(X + \Delta) \approx g(X) + \tr(X^{-1} \Delta) \]
hence $d g(X) / d X = X^{-1}$ (note: consider eigenvalues of $X^{-1/2} \Delta X^{-1/2}$); 
\item Use these formal derivatives with respect to positive semi-definite matrix variables to show that the maximum likelihood estimator of $\Sigma$ is 
\[ \hat{\Sigma}^{MLE}_n = S_n. \]
\end{enumerate}
A reference for (b) and (c) can be found in Convex Optimization, by Boyd and Vandenbergh, examples in Appendix A.4.1 and A.4.3:

\url{https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf}


\item {\em Shrinkage:} Suppose $y\sim \NN(\mu, I_p)$. 

\begin{enumerate}
\item Consider the Ridge regression
\[ \min_{\mu} \frac{1}{2}\|y - \mu \|_2^2 +\frac{\lambda}{2} \|\mu\|_2^2. \]
Show that the solution is given by 
\[ \hat{\mu}^{ridge}_i = \frac{1}{1+\lambda} y_i .\]
Compute the risk (mean square error) of this estimator. The risk of MLE is given when $C=I$. 
\item Consider the LASSO problem,
\[ \min_{\mu} \frac{1}{2}\|y - \mu \|_2^2 +\lambda \|\mu\|_1. \]
Show that the solution is given by Soft-Thresholding
\[ \hat{\mu}^{soft}_i = \mu_{soft}( y_i;\lambda) := \sign( y_i ) (|y_i |- \lambda)_+. \]
For the choice $\lambda = \sqrt{2\log p}$, show that the risk is bounded by
\[ \E\|\hat{\mu}^{soft}(y) -\mu\|^2  \leq 1+ (2\log p + 1) \sum_{i=1}^p \min(\mu_i^2,1).\]
Under what conditions on $\mu$, such a risk is smaller than that of MLE?
Note: see Gaussian Estimation by Iain Johnstone, Lemma 2.9 and the reasoning before it.
\item Consider the $l_0$ regularization 
\[ \min_{\mu} \|y - \mu \|_2^2 +\lambda^2 \|\mu\|_0, \]
where $\|\mu\|_0 := \sum_{i=1}^p I(\mu_i \neq 0)$. Show that the solution is given by Hard-Thresholding
\[ \hat{\mu}^{hard}_i = \mu_{hard}( y_i;\lambda) := y_i  I(|y_i|>\lambda).\]
Rewriting $\hat{\mu}^{hard}(y) = (1-g(y))y$, is $g(y)$ weakly differentiable? Why? 
\item Consider the James-Stein Estimator 
\[ \hat{\mu}^{JS}(y) = \left( 1 - \frac{\alpha}{\|y\|^2} \right) y. \]
Show that the risk is 
\[ \E\|\hat{\mu}^{JS}(y) - \mu\|^2 = \E U_\alpha(y) \]
where $U_\alpha(y) = p - (2\alpha(p-2) - \alpha^2) / \|y\|^2$. Find the optimal $\alpha^\ast=\arg\min_\alpha U_\alpha(y)$. Show that for $p>2$, the risk of James-Stein Estimator is smaller than that of MLE for all $\mu\in \R^p$. 
\item In general, an odd monotone unbounded function $\Theta:\R\to\R$ defined by $\Theta_\lambda(t)$ with parameter $\lambda \geq 0$ is called \emph{shrinkage} rule, if it satisfies 
\subitem[shrinkage] $0\leq \Theta_\lambda(|t|) \leq |t|$; 
\subitem[odd] $\Theta_\lambda(-t) = - \Theta_\lambda(t)$;
\subitem[monotone] $\Theta_\lambda(t) \leq \Theta_\lambda(t^\prime)$ for $t\leq t^\prime$;
\subitem[unbounded] $\lim_{t\to \infty} \Theta_\lambda(t)=\infty$. \\
Which rules above are shrinkage rules? 
\end{enumerate}

\item {\em *Necessary Condition for Admissibility of Linear Estimators}. Consider linear estimator for $y\sim \NN(\mu,\sigma^2 I_p)$
\[ \hat{\mu}_C(y) = C y. \]
Show that $\hat{\mu}_C$ is admissible only if
		\begin{enumerate}
			\item $C$ is symmetric;
			\item $0 \le \rho_i(C) \le 1$ (where $\rho_i(C)$ are eigenvalues of $C$);
			\item $\rho_i(C) = 1$ for at most two $i$.
		\end{enumerate}	
		These conditions are satisfied for MLE estimator when $p=1$ and $p=2$. 
			
Reference: Theorem 2.3 in Gaussian Estimation by Iain Johnstone,\\
\url{http://statweb.stanford.edu/~imj/Book100611.pdf}

%\item {\em James Stein Estimator for $p=1$:} 
%
%From Theorem 3.1 in the lecture notes, we know that MLE $\hat{\mu} = Y$ is admissible when $p=1 \text{ or } 2$. However if we use SURE to calculate the risk of James Stein Estimator,
%\[
%	R(\hat{\mu}^{\text{JS}},\mu) = \E U(Y) 
%	= p - \E_\mu \frac{(p-2)^2}{\lVert Y \rVert^2}
%	< p = R(\hat{\mu}^{\text{MLE}},\mu)
%\]
%it seems that for $p=1$ James Stein Estimator should still has lower risk than MLE for any $\mu$.
% Explain what violates the above calculation for $p=1$.
%

\end{enumerate}

\end{document}


